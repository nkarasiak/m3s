{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample: Parallel Processing and Streaming with M3S.\n\nDemonstrates the parallel processing capabilities including:\n1. Dask distributed computing\n2. GPU acceleration (if available)\n3. Streaming data processing\n4. Multi-grid batch operations\n5. Performance monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport warnings\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom shapely.geometry import Point\n\nfrom m3s import (\n    GeohashGrid,\n    H3Grid,\n    MGRSGrid,\n    ParallelConfig,\n    ParallelGridEngine,\n    create_data_stream,\n    parallel_intersect,\n    stream_grid_processing,\n)\n\n\ndef create_large_test_dataset(n_points: int = 1000) -> gpd.GeoDataFrame:\n    \"\"\"Create a large test dataset for parallel processing demonstration.\"\"\"\n    print(f\"Creating test dataset with {n_points:,} points...\")\n\n    # Generate random points across continental US\n    lats = np.random.uniform(25, 49, n_points)  # Continental US latitude range\n    lons = np.random.uniform(-125, -66, n_points)  # Continental US longitude range\n\n    geometries = [Point(lon, lat) for lat, lon in zip(lons, lats)]\n\n    # Add some sample attributes\n    data = {\n        \"id\": range(n_points),\n        \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], n_points),\n        \"value\": np.random.uniform(0, 100, n_points),\n        \"timestamp\": pd.date_range(\"2024-01-01\", periods=n_points, freq=\"1min\"),\n    }\n\n    return gpd.GeoDataFrame(data, geometry=geometries, crs=\"EPSG:4326\")\n\n\ndef benchmark_processing_methods(gdf: gpd.GeoDataFrame):\n    \"\"\"Benchmark different processing methods.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BENCHMARKING PROCESSING METHODS\")\n    print(\"=\" * 60)\n\n    grid = GeohashGrid(precision=5)\n    sample_size = min(1000, len(gdf))  # Use smaller sample for benchmarking\n    sample_gdf = gdf.sample(n=sample_size).reset_index(drop=True)\n\n    print(f\"Using sample of {sample_size:,} points for benchmarking...\")\n\n    # 1. Standard single-threaded processing\n    print(\"\\n1. Standard Processing (Single-threaded)\")\n    start_time = time.time()\n    result_standard = grid.intersects(sample_gdf)\n    standard_time = time.time() - start_time\n    print(f\"   Time: {standard_time:.2f} seconds\")\n    print(f\"   Results: {len(result_standard):,} grid cells\")\n\n    # 2. Parallel processing with threading\n    print(\"\\n2. Parallel Processing (Threading)\")\n    config_threaded = ParallelConfig(use_dask=False, use_gpu=False, chunk_size=1000)\n    start_time = time.time()\n    result_parallel = parallel_intersect(grid, sample_gdf, config_threaded)\n    parallel_time = time.time() - start_time\n    print(f\"   Time: {parallel_time:.2f} seconds\")\n    print(f\"   Results: {len(result_parallel):,} grid cells\")\n    print(f\"   Speedup: {standard_time / parallel_time:.2f}x\")\n\n    # 3. Dask processing (if available)\n    try:\n        print(\"\\n3. Dask Distributed Processing\")\n        config_dask = ParallelConfig(use_dask=True, use_gpu=False, chunk_size=1000)\n        engine = ParallelGridEngine(config_dask)\n\n        if engine._client:\n            start_time = time.time()\n            result_dask = engine.intersect_parallel(grid, sample_gdf)\n            dask_time = time.time() - start_time\n            print(f\"   Time: {dask_time:.2f} seconds\")\n            print(f\"   Results: {len(result_dask):,} grid cells\")\n            print(f\"   Speedup: {standard_time / dask_time:.2f}x\")\n        else:\n            print(\"   Dask client not available - skipping\")\n    except Exception as e:\n        print(f\"   Dask processing failed: {e}\")\n\n    # 4. GPU processing (if available)\n    try:\n        print(\"\\n4. GPU Processing (RAPIDS)\")\n        config_gpu = ParallelConfig(use_dask=False, use_gpu=True, chunk_size=1000)\n        engine = ParallelGridEngine(config_gpu)\n\n        start_time = time.time()\n        result_gpu = engine.intersect_parallel(grid, sample_gdf)\n        gpu_time = time.time() - start_time\n        print(f\"   Time: {gpu_time:.2f} seconds\")\n        print(f\"   Results: {len(result_gpu):,} grid cells\")\n        print(f\"   Speedup: {standard_time / gpu_time:.2f}x\")\n    except Exception as e:\n        print(f\"   GPU processing not available: {e}\")\n\n\ndef demonstrate_streaming_processing(gdf: gpd.GeoDataFrame):\n    \"\"\"Demonstrate streaming data processing.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STREAMING DATA PROCESSING\")\n    print(\"=\" * 60)\n\n    grid = H3Grid(resolution=6)\n    chunk_size = 2000\n\n    print(f\"Processing {len(gdf):,} points in chunks of {chunk_size:,}...\")\n\n    # Create data stream\n    data_stream = create_data_stream(gdf, chunk_size=chunk_size)\n\n    # Track processing with callback\n    processed_chunks = []\n    total_cells = 0\n\n    def chunk_callback(chunk_result):\n        nonlocal total_cells\n        processed_chunks.append(len(chunk_result))\n        total_cells += len(chunk_result)\n        print(\n            f\"   Processed chunk: {len(chunk_result):,} cells (Total: {total_cells:,})\"\n        )\n\n    config = ParallelConfig(use_dask=False, use_gpu=False, n_workers=4)\n\n    start_time = time.time()\n    result = stream_grid_processing(\n        grid, data_stream, config, output_callback=chunk_callback\n    )\n    processing_time = time.time() - start_time\n\n    print(\"\\nStreaming Processing Complete:\")\n    print(f\"   Total time: {processing_time:.2f} seconds\")\n    print(f\"   Chunks processed: {len(processed_chunks)}\")\n    print(f\"   Total cells: {len(result):,}\")\n    print(f\"   Processing rate: {len(gdf) / processing_time:.0f} points/second\")\n\n\ndef demonstrate_multi_grid_processing(gdf: gpd.GeoDataFrame):\n    \"\"\"Demonstrate processing with multiple grid systems simultaneously.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"MULTI-GRID BATCH PROCESSING\")\n    print(\"=\" * 60)\n\n    # Create different grid systems\n    grids = [\n        GeohashGrid(precision=4),\n        GeohashGrid(precision=5),\n        H3Grid(resolution=5),\n        H3Grid(resolution=6),\n        MGRSGrid(precision=1),\n    ]\n\n    grid_names = [\"geohash_p4\", \"geohash_p5\", \"h3_res5\", \"h3_res6\", \"mgrs_p1\"]\n\n    # Use subset for multi-grid demo\n    sample_size = min(2000, len(gdf))\n    sample_gdf = gdf.sample(n=sample_size).reset_index(drop=True)\n\n    print(\n        f\"Processing {sample_size:,} points with {len(grids)} different grid systems...\"\n    )\n\n    config = ParallelConfig(use_dask=False, use_gpu=False, n_workers=4)\n    engine = ParallelGridEngine(config)\n\n    start_time = time.time()\n    results = engine.batch_intersect_multiple_grids(grids, sample_gdf, grid_names)\n    processing_time = time.time() - start_time\n\n    print(\"\\nMulti-Grid Processing Complete:\")\n    print(f\"   Total time: {processing_time:.2f} seconds\")\n    print(f\"   Grids processed: {len(results)}\")\n\n    print(\"\\nResults by grid system:\")\n    for name, result in results.items():\n        print(f\"   {name:12}: {len(result):,} cells\")\n\n    return results\n\n\ndef visualize_multi_grid_results(results: dict, sample_gdf: gpd.GeoDataFrame):\n    \"\"\"Visualize results from multiple grid systems.\"\"\"\n    print(\"\\nCreating visualization of multi-grid results...\")\n\n    # Select subset for visualization\n    viz_bounds = (-100, 35, -95, 40)  # Focus on central US area\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    fig.suptitle(\"Multi-Grid System Comparison\", fontsize=16)\n\n    axes = axes.flatten()\n\n    # Plot original points\n    ax = axes[0]\n    sample_gdf.plot(ax=ax, markersize=0.5, alpha=0.6, color=\"red\")\n    ax.set_xlim(viz_bounds[0], viz_bounds[2])\n    ax.set_ylim(viz_bounds[1], viz_bounds[3])\n    ax.set_title(\"Original Points\")\n    ax.grid(True, alpha=0.3)\n\n    # Plot each grid system\n    for idx, (name, result) in enumerate(list(results.items())[:5]):\n        ax = axes[idx + 1]\n\n        if len(result) > 0:\n            # Filter to visualization bounds\n            result_subset = result.cx[\n                viz_bounds[0] : viz_bounds[2], viz_bounds[1] : viz_bounds[3]\n            ]\n\n            if len(result_subset) > 0:\n                result_subset.plot(\n                    ax=ax,\n                    facecolor=\"lightblue\",\n                    edgecolor=\"blue\",\n                    alpha=0.6,\n                    linewidth=0.5,\n                )\n                ax.set_title(f\"{name}\\n({len(result_subset):,} cells in view)\")\n            else:\n                ax.set_title(f\"{name}\\n(No cells in view)\")\n        else:\n            ax.set_title(f\"{name}\\n(No results)\")\n\n        ax.set_xlim(viz_bounds[0], viz_bounds[2])\n        ax.set_ylim(viz_bounds[1], viz_bounds[3])\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef demonstrate_performance_monitoring():\n    \"\"\"Demonstrate performance monitoring capabilities.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PERFORMANCE MONITORING\")\n    print(\"=\" * 60)\n\n    # Try different configurations\n    configs = [\n        (\"Threading\", ParallelConfig(use_dask=False, use_gpu=False, n_workers=2)),\n        (\n            \"Threading (4 workers)\",\n            ParallelConfig(use_dask=False, use_gpu=False, n_workers=4),\n        ),\n        (\"Dask\", ParallelConfig(use_dask=True, use_gpu=False, n_workers=2)),\n    ]\n\n    for name, config in configs:\n        print(f\"\\n{name} Configuration:\")\n        engine = ParallelGridEngine(config)\n        stats = engine.get_performance_stats()\n\n        for key, value in stats.items():\n            if isinstance(value, dict):\n                print(f\"   {key}:\")\n                for subkey, subvalue in value.items():\n                    print(f\"      {subkey}: {subvalue}\")\n            else:\n                print(f\"   {key}: {value}\")\n\n\ndef main():\n    \"\"\"Run main demonstration.\"\"\"\n    print(\"M3S Parallel Processing Demonstration\")\n    print(\"=\" * 60)\n\n    # Create test dataset\n    gdf = create_large_test_dataset(n_points=10000)  # Moderate size for demo\n\n    print(f\"Test dataset created: {len(gdf):,} points\")\n    print(f\"Bounds: {gdf.total_bounds}\")\n    print(f\"Memory usage: ~{gdf.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n\n    # Run demonstrations\n    try:\n        benchmark_processing_methods(gdf)\n        demonstrate_streaming_processing(gdf)\n\n        # Multi-grid processing with visualization\n        sample_gdf = gdf.sample(n=1000).reset_index(drop=True)\n        results = demonstrate_multi_grid_processing(sample_gdf)\n\n        # Show visualization\n        try:\n            visualize_multi_grid_results(results, sample_gdf)\n        except Exception as e:\n            print(f\"Visualization failed: {e}\")\n\n        demonstrate_performance_monitoring()\n\n    except KeyboardInterrupt:\n        print(\"\\nDemo interrupted by user\")\n    except Exception as e:\n        print(f\"\\nDemo failed with error: {e}\")\n        import traceback\n\n        traceback.print_exc()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"DEMO COMPLETE\")\n    print(\"=\" * 60)\n\n    print(\"\\nKey Features Demonstrated:\")\n    print(\"\u2713 Parallel processing with threading and Dask\")\n    print(\"\u2713 Streaming data processing for large datasets\")\n    print(\"\u2713 Multi-grid system batch operations\")\n    print(\"\u2713 Performance monitoring and benchmarking\")\n    print(\"\u2713 Graceful fallbacks for missing dependencies\")\n\n    print(\"\\nTo enable additional features:\")\n    print(\"\u2022 Install Dask: pip install 'dask[complete]' distributed\")\n    print(\"\u2022 Install RAPIDS: pip install cupy cudf cuspatial\")\n\n\nif __name__ == \"__main__\":\n    # Suppress warnings for cleaner demo output\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Dask.*\")\n\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}